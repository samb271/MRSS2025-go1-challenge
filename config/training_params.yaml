task: MRSS-Velocity-Go1-v0  # Name of the task.

headless: true  # Run simulation without rendering a GUI (faster, better for training on servers)
max_iterations: 5000  # Adjust this to train for longer: 1.5k should give you a working policy for rough terrain

seed: 1 # Performance should be somewhat consistent across seeds

video: false  # Record videos during training.
video_length: 10  # Length of the recorded video (in steps).
video_interval: 10  # Interval between video recordings (in steps).

run_name: go1_locomotion  # Run name suffix to the log directory.
resume: false  # Whether to resume from a checkpoint.
#load_run: LOAD_RUN   # Name of the run folder to resume from.
#checkpoint: CHECKPOINT  # Checkpoint file to resume from.
logger: wandb  # Logger module to use. {neptune, wandb, tensorboard}
log_project_name: MRSS25  # Name of the logging project when using wandb or neptune.

env:
  scene:
    num_envs: 8192  # Number of environments to generate terrain for and run in parallel.

    terrain:
      max_init_terrain_level: 10  # Maximum initial difficulty level of terrains for curriculum learning

      terrain_generator: 
        # Each column is assigned a random terrain type from the sub_terrains.
        num_cols: 30  # Number of terrain columns (patches) in the scene. Default is 20.
        num_rows: 20  # Number of terrain rows (patches) in the scene. Default is 10.
        
        # The difficulty linearly increases along the rows if curriculum is enabled.
        curriculum: True  # Enable terrain difficulty progression over time
        
        # You can change the proportions of the different terrains
        # See the different possible terrains here https://isaac-sim.github.io/IsaacLab/main/source/api/lab/isaaclab.terrains.html#terrain-generator
        sub_terrains:
          flat: 
            proportion: 0.1

          pyramid_stairs:
            proportion: 0.18

          pyramid_stairs_inv:
            proportion: 0.18

          boxes:
            proportion: 0.18

          random_rough: 
            proportion: 0.18

          pyramid_slope: 
            proportion: 0.18

          pyramid_slope_inv: 
            proportion: 0.
        
      # terrain_generator: 
      #   sub_terrains: 'null'
      #   border_height: 1.0  # Height of border around the terrain patch
      #   border_width: 20.0  # Width of border around the terrain patch
      #   difficulty_range: [ 0.0, 1.0 ]  # Range of terrain difficulties to sample from
      #   horizontal_scale: 0.1  # Horizontal scaling factor for terrain features
      #   size: [ 8.0, 8.0 ]  # Size of each terrain patch (meters)
      #   slope_threshold: 0.75  # Max slope allowed for terrain generation
      #   use_cache: False  # Use cached terrains or regenerate every time
      #   vertical_scale: 0.005  # Vertical scaling factor for heightmap features
    
  rewards:
    # Smooth control changes
    action_rate_l2:
      weight: -0.05      # Enough to smooth actions, still responsive
    
    # Stability (reduce wobble & roll)
    ang_vel_xy_l2:
      weight: -0.1
    
    # Smooth motion at joint level
    dof_acc_l2:
      weight: -5e-06     # Big enough to matter; tune later
    
    # Prevent hyperextension
    dof_pos_limits:
      weight: -0.05
    
    # Energy efficiency without crippling agility
    dof_torques_l2:
      weight: -0.0005
    
    # Encourage stepping without promoting hopping
    feet_air_time:
      weight: 0.3
    
    # Keep torso roughly level
    flat_orientation_l2:
      weight: -0.4
    
    # Suppress vertical bouncing
    lin_vel_z_l2:
      weight: -1.0
    
    # Angular velocity tracking (turning control)
    track_ang_vel_z_exp:
      weight: 0.65
    
    # Main driver: forward velocity tracking
    track_lin_vel_xy_exp:
      weight: 2.0


  events:
    add_base_mass:
      params:
        mass_distribution_params: [ -1.0, 3.0 ]  # Range for randomizing the base mass

    base_external_force_torque:
      params:
        force_range: [ 0.0, 15.0 ]  # Range for randomizing the external force applied to the base
        torque_range: [ 0.0, 2.0 ]  # Range for randomizing the external torque applied to the base

    push_robot:
      mode: "interval"
      interval_range_s: [10.0, 15.0]     # Push every 10â€“15 seconds
      params:
        velocity_range:
          x: [-0.5, 0.5]                 # Push in m/s along x-axis
          y: [-0.5, 0.5]                 # Push in m/s along y-axis


algorithm:
  class_name: 'PPO'  # Name of the RL algorithm to use (Proximal Policy Optimization)
  clip_param: 0.2  # Clipping parameter for PPO objective
  desired_kl: 0.01  # Target KL divergence between old and new policies
  entropy_coef: 0.01  # Coefficient for entropy bonus (encourages exploration)
  gamma: 0.99  # Discount factor for future rewards
  lam: 0.95  # GAE lambda parameter for bias-variance tradeoff
  learning_rate: 0.0005  # Initial learning rate for optimizer
  max_grad_norm: 1.0  # Maximum gradient norm for clipping
  normalize_advantage_per_mini_batch: True  # Whether to normalize advantages per mini-batch
  num_learning_epochs: 5  # Number of passes over data per training iteration
  num_mini_batches: 128  # Number of mini-batches per epoch
  rnd_cfg: null  # Configuration for Random Network Distillation (not used here)
  schedule: 'adaptive'  # Learning rate schedule type (adaptive vs fixed)
  symmetry_cfg: null  # Optional symmetry loss configuration (not used here)
  use_clipped_value_loss: True  # Use value function clipping in PPO
  value_loss_coef: 1.0  # Coefficient for value function loss
